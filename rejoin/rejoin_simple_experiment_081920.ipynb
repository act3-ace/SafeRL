{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# get rid of tensorflow annoying compatability warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Ray Temp Directory\n",
    "This will probably be different depending on your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_dir = '/home/aaco/tmp/ray'  # for docker container\n",
    "temp_dir = '~/tmp/ray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.rllib.utils.spaces.space_utils import flatten_to_single_ndarray\n",
    "from ray.tune.logger import pretty_print\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import utm\n",
    "import yaml\n",
    "\n",
    "from act3.agents.utilities.plugin_loader import get_integration_base\n",
    "from act3.agents.utilities.util import expand_path, mkdir_p\n",
    "from act3.environments.domain_randomization.dr_module import DomainRandomization\n",
    "from act3.simulators.afsim.afsim_integration_controls import (\n",
    "    AfsimStickAndThrottleP6DOFController,\n",
    ")\n",
    "from act3.simulators.afsim.afsim_integration_sensors import (\n",
    "    AfsimFuelSensor,\n",
    "    AfsimGloadSensor,\n",
    "    AfsimOrientationRateSensor,\n",
    "    AfsimSpeedKcasSensor,\n",
    ")\n",
    "from act3.simulators.base_integration import BaseIntegration, BasePlatform\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False, temp_dir=temp_dir, webui_host='127.0.0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Environment Class\n",
    "This is a simple environment class. It interfaces with AFSIM and uses the Domain Randomization module to initialize the scenerio on each reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Class\n",
    "class RejoinDREnv(gym.Env):\n",
    "    \"\"\"Simple rejoin task testing environment using AFSIM 2.6.\"\"\"\n",
    "    \n",
    "    def __init__(self, env_config=None):\n",
    "        \"\"\"Initialize environment with provided configuration.\n",
    "        \n",
    "        The configuration specifies how long an episode should be\n",
    "        in steps, what the step reward value is, the configuration\n",
    "        for the simulation and specification for the initialization\n",
    "        module that sets the state of the environment at the start\n",
    "        of each episode.\n",
    "        \"\"\"\n",
    "        # ******* Setup environment stuff ******\n",
    "        # TODO: This will probably change\n",
    "        \n",
    "        # setup info dictionary\n",
    "        self._info = {}\n",
    "        \n",
    "        # maximum length of episode\n",
    "        if 'horizon' in env_config.keys():\n",
    "            self._horizon = env_config['horizon']\n",
    "        else:\n",
    "            self._horizon = 1000\n",
    "            \n",
    "        self._info['horizon'] = self._horizon\n",
    "            \n",
    "        # setup for tracking episode length\n",
    "        self._current_step = 0\n",
    "        \n",
    "        # setup step reward\n",
    "        if 'step_reward' in env_config.keys():\n",
    "            self._step_reward = env_config['step_reward']\n",
    "        else:\n",
    "            self._step_reward = 0.001\n",
    "            \n",
    "        self._info['step_reward'] = self._step_reward\n",
    "            \n",
    "        # TODO: this is probably in the env_config somewhere\n",
    "        self.range_target = 6000 / 3.2808  # target range difference in meters\n",
    "        self.alt_target = 0 / 3.2808  # target altitude difference in meters\n",
    "        \n",
    "        # ******* End setup environment stuff ******\n",
    "        \n",
    "        # ****** Start Simulation Setup ******\n",
    "        # TODO: This should be a module that wraps the\n",
    "        # AFSIM/PAW stuff, that way we could use this environment\n",
    "        # directly with another simulator (e.g. Unity) and just\n",
    "        # define an abstract class that enforces the interface\n",
    "        \n",
    "        # set the AFSIM simulator\n",
    "        if 'sim_config' not in env_config.keys():\n",
    "            raise (ValueError, 'Environment config must contain a sim_config!')\n",
    "        self._sim_config = env_config['sim_config']\n",
    "        \n",
    "        base_output_path = env_config['sim_config']['scen']['output_path']\n",
    "        try:\n",
    "            worker_index = str(env_config.worker_index).zfill(4)\n",
    "        except AttributeError:\n",
    "            worker_index = str(0).zfill(4)\n",
    "            \n",
    "        self._output_path = os.path.join(base_output_path,\n",
    "                                         worker_index)\n",
    "        \n",
    "        mkdir_p(self._output_path)\n",
    "        \n",
    "        self._sim_config['scen']['output_path'] = self._output_path\n",
    "      \n",
    "        # setup controller for afsim, this sets up basic hotas commands\n",
    "        self._sim_config[\"platform_configs\"] = {\n",
    "            \"blue0\": {\n",
    "                \"controller_class_map\": {\n",
    "                    \"mover\": [(AfsimStickAndThrottleP6DOFController, {}),]\n",
    "                },\n",
    "                \"sensor_class_map\": {\n",
    "                    \"None\": [\n",
    "                        (AfsimOrientationRateSensor, {}),\n",
    "                        (AfsimFuelSensor, {}),\n",
    "                        (AfsimSpeedKcasSensor, {}),\n",
    "                        (AfsimGloadSensor, {}),\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # AFSIM 2.6\n",
    "        sim_class = get_integration_base(env_config['simulator'])\n",
    "        self._sim = sim_class(config=self._sim_config)\n",
    "       \n",
    "        # TODO: Set the framerate of the simulation here\n",
    "        \"\"\"\n",
    "        if 'frame_rate' in self._sim_config['scen'].keys():\n",
    "            self._sim.update_rate = 1.0 / self._sim_config['scen']['frame_rate']\n",
    "        else:\n",
    "            self._sim.update_rate = (1.0 / 10.0)\n",
    "        \"\"\"\n",
    "        \n",
    "        self._sim.reset_simulation(self._sim_config)\n",
    "        \n",
    "        # ******* End Simulation Setup *******\n",
    "         \n",
    "        # ******* Setup Helper Classes *******\n",
    "        self._current_episode = 0\n",
    "        self._info['current_episode'] = self._current_episode\n",
    "            \n",
    "        # setup AFSIM to rllib observation converter\n",
    "        self._obs_processor = AfsimStateToObs()\n",
    "        \n",
    "        # setup domain randomization module\n",
    "        self._initializer = RejoinInitialConditions(env_config)\n",
    "        self._info['phase'] = self._initializer.phase\n",
    "        \n",
    "        # setup action processor\n",
    "        self._action_processor = AfsimHOTASController(env_config)\n",
    "        \n",
    "        # setup reward calculation module\n",
    "        self._reward_calculator = RejoinRewardCalculator(env_config)\n",
    "        \n",
    "        # setup done checker module\n",
    "        self._done_checker = RejoinDoneChecker(env_config)\n",
    "        \n",
    "        # ******* Setup Helper Classes *******\n",
    "        \n",
    "        # *** setup action and observation spaces ***\n",
    "        self._setup_spaces()\n",
    "        \n",
    "    def set_phase(self, phase):\n",
    "        \"\"\"Set phase for UDR module.\n",
    "        \n",
    "        The phase governs how the initial states of the agent\n",
    "        and lead aircraft are set at the start of each episode.\n",
    "        \"\"\"\n",
    "        self._info['phase'] = phase\n",
    "        self._initializer.set_phase(phase) \n",
    "       \n",
    "    def _setup_spaces(self):\n",
    "        \"\"\"Setup action and observation spaces.\n",
    "        \n",
    "        These spaces are generated by the respective helper\n",
    "        classes.\n",
    "        \"\"\"\n",
    "        self.action_space = self._action_processor.get_action_space()\n",
    "        \n",
    "        action_obs_space = self._action_processor.get_observation_space()\n",
    "        \n",
    "        self.observation_space = self._obs_processor.get_observation_space(action_obs_space)\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"Peform a single step of the environment.\n",
    "        \n",
    "        This will take an action generated by the policy and\n",
    "        apply it to the agent in the simulation via the\n",
    "        action processor helper class. The done conditions\n",
    "        for the agent will then be checked and the reward\n",
    "        for the agent's action will be generated based on\n",
    "        the current state of the environment. Finally,\n",
    "        the observation, reward, done and environment info\n",
    "        will be returned to the policy for learning.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : tuple[float]\n",
    "            Action computed by the rllib policy\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[dict, dict, dict, dict]\n",
    "            Tuple containing [observation, reward, done, info]\n",
    "        \"\"\"\n",
    "        # apply the action to the simulator\n",
    "        platforms = self._sim.get_state()\n",
    "        \n",
    "        self._action_processor(platforms, action)\n",
    "       \n",
    "        sim_time = self._sim.advance_update()  # advance one step\n",
    "        self._current_step += 1\n",
    "        \n",
    "        self._info['current_step'] = self._current_step\n",
    "        \n",
    "        # TODO: Does this need to happen? or are the platforms\n",
    "        # just references?\n",
    "        # try:\n",
    "        # if ^ doesn't fail, then next line not needed\n",
    "        sim_platforms = self._sim.get_state()\n",
    "        #assert platforms == sim_platforms\n",
    "        # if ^ doesn't fail, then next line not needed\n",
    "       \n",
    "        sim_state = {}\n",
    "        for p in sim_platforms:\n",
    "            sim_state[p.name] = p\n",
    "            \n",
    "        # might not need above for loop based on above test\n",
    "            \n",
    "        # popluate info with the current simulation state as\n",
    "        # a StateDict\n",
    "        # TODO: This should just be a dictionary to decouple\n",
    "        # it from AFSIM-specific stuff\n",
    "        # NOTE: the PAW encrypts the platform objects, so they\n",
    "        # can't be pickled, if added to info they break everything\n",
    "        # self._info['sim_state'] = sim_state\n",
    "       \n",
    "        self._info['action_state'] = self._action_processor.get_observation(sim_platforms)\n",
    "        \n",
    "        # TODO: add unscaled observation to info\n",
    "        # get scaled observations\n",
    "        obs = self._obs_processor(sim_state=sim_platforms,\n",
    "                                  sim_time=sim_time,\n",
    "                                  info=self._info)\n",
    "        \n",
    "        # process done state\n",
    "        done = self._done_checker(self._info)\n",
    "        \n",
    "        # calculate reward from environment info\n",
    "        reward = self._reward_calculator(self._info)\n",
    "        \n",
    "        return obs, reward, done, self._info\n",
    "        #return obs, reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment and simulation state.\n",
    "        \n",
    "        This will archive the AFSIM aer file generated for the\n",
    "        current episode. It will also reset all helper classes\n",
    "        to their initial states.\n",
    "        It will then reset the current state of the simulation\n",
    "        environment and set the initial state of each platform\n",
    "        in the environment based on the domain randomization\n",
    "        module contained in self._initializer.\n",
    "        \"\"\"\n",
    "       \n",
    "        # TODO: This should be a part of the simulation module\n",
    "        # as it is afsim specific\n",
    "        # archive aer file\n",
    "        temp = os.path.join(self._output_path, \"replay.aer\")\n",
    "        if os.path.isfile(temp):\n",
    "            os.rename(\n",
    "                temp,\n",
    "                os.path.join(\n",
    "                    self._output_path,\n",
    "                    (f\"replay_{str(self._current_episode).zfill(8)}_\"\n",
    "                     f\"{str(self._current_step).zfill(8)}.aer\"),\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "        # reset AFSIM \n",
    "        self._sim.reset_simulation(self._sim_config,\n",
    "                                   self.update_platform_init)\n",
    "        \n",
    "        # reset observation processor\n",
    "        self._obs_processor.reset()\n",
    "        \n",
    "        # reset reward calculator\n",
    "        self._reward_calculator.reset()\n",
    "        \n",
    "        # reset done checker\n",
    "        self._done_checker.reset()\n",
    "       \n",
    "        # get state information\n",
    "        sim_time = 0.0\n",
    "        sim_platforms = self._sim.get_state()\n",
    "        \n",
    "        # reset step counter\n",
    "        self._current_step = 0\n",
    "        self._info['current_step'] = self._current_step\n",
    "       \n",
    "        # increment episode counter\n",
    "        self._current_episode += 1\n",
    "        self._info['current_episode'] = self._current_episode\n",
    "        \n",
    "        # initialize action state\n",
    "        self._info['action_state'] = self._action_processor.get_observation(sim_platforms)\n",
    "        \n",
    "        # reset num steps inside bubble counter\n",
    "        self._info['num_steps_inside_bubble'] = 0\n",
    "        \n",
    "        return self._obs_processor(sim_state=sim_platforms,\n",
    "                                   sim_time=sim_time,\n",
    "                                   info=self._info)\n",
    "    \n",
    "    def update_platform_init(self, state):\n",
    "        \"\"\"Update sim configuration based on the currently set phase.\n",
    "        \n",
    "        This will position the agent and the lead aircraft, orient them\n",
    "        and set their speeds based on the configuration and phase information\n",
    "        processed by the initializer helper class.\n",
    "        \"\"\"\n",
    "        self._initializer(state)\n",
    "    \n",
    "    @property\n",
    "    def simulator(self):\n",
    "        \"\"\"The simulator used by this environment.\"\"\"\n",
    "        return self._sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Processor\n",
    "\n",
    "This takes the state of the simulation and provides an observation dictionary that can be ingested by rllib's policy network. This class is used by RejoinDREnv and handles all observation related processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afsim observation to (scaled) rllib observation\n",
    "class AfsimStateToObs:\n",
    "    \"\"\"Convert AFSIM 2.6 simulation state to rllib observation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Setup observation processor state variables.\n",
    "        \n",
    "        Currently this just initializes the internal alt_diff\n",
    "        variable that stores the altitude difference and time\n",
    "        from the previous conversion.\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "       \n",
    "        # scale factors\n",
    "        # TODO: These values should be in a config or something\n",
    "        self._scale_factors = {}\n",
    "        self._scale_factors['altitude'] = [7500, 3000]\n",
    "        self._scale_factors['velocity'] = [[0, 350], [0, 350], [0, 350]]\n",
    "        # TODO: Figure out good values\n",
    "        self._scale_factors['acceleration'] = [[0, 3500], [0, 3500], [0, 3500]]\n",
    "        # TODO: Figure out good values\n",
    "        self._scale_factors['angular_velocity'] = [[0, 3000], [0, 3000], [0, 3000]]\n",
    "        # TODO: Adjust these - decrease variances?\n",
    "        self._scale_factors['dist_from_goal'] = [[0, 10000], [0, 10000], [0, 7500]]\n",
    "        # TODO: Figure out good values\n",
    "        # TODO: these variances are too high\n",
    "        self._scale_factors['dist_from_goal_rates'] = [[0, 10000], [0, 10000], [0, 7500]]\n",
    "        # TODO: Same as dist from goal, should have a larger mean and smaller viance (isn't negative)\n",
    "        self._scale_factors['range_to_goal'] = [0, 10000]\n",
    "        # TODO: ensure this value is reasonable\n",
    "        self._scale_factors['range_to_goal_rate'] = [0, 600]\n",
    "        self._scale_factors['lead_rel_velocity'] = [[0, 100], [0, 100], [0, 100]]\n",
    "        # TODO: this probably needs a non-zero mean and smaller variance, can't be negative\n",
    "        self._scale_factors['range_to_lead'] = [0, 10000]\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset state of the processor.\"\"\"\n",
    "        \n",
    "        self._prev_time = 0.0\n",
    "        self._prev_dist = 0.0\n",
    "        self._prev_e_diff = 0.0\n",
    "        self._prev_n_diff = 0.0\n",
    "        self._prev_a_diff = 0.0\n",
    "        self._prev_r2g = 0.0\n",
    "        self._prev_ang_vel = [0.0, 0.0, 0.0]\n",
    "   \n",
    "    def __call__(self, sim_state, sim_time, info=None):\n",
    "        \"\"\"Get observation from simulation state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        TODO: Update this to reflect it is now a Tuple[AfsimPlatform, ...]\n",
    "        sim_state : StateDict\n",
    "            StateDict object containing entries for each entity in\n",
    "            the simulation. For this environment, it is assumed to\n",
    "            contain a blue0 (our agent) and red0 (the lead aircraft).\n",
    "        sim_time : float\n",
    "            Current time of the simulation.\n",
    "        info : dict\n",
    "            Information dictionary from the environment, optional\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        OrderedDict\n",
    "            Dictionary of scaled observations\n",
    "        \"\"\"\n",
    "        # get agent's state information\n",
    "        platforms = {}\n",
    "        for p in sim_state:\n",
    "            platforms[p.name] = p\n",
    "            \n",
    "        current_time_diff = sim_time - self._prev_time\n",
    "            \n",
    "        # TODO: Most of the below should be refactored into functions\n",
    "            \n",
    "        blue = platforms['blue0']\n",
    "        \n",
    "        blue_alt = blue.position[2]\n",
    "        # get orientation of aircraft in yaw, pitch and roll in radians\n",
    "        blue_orientation = blue.orientation\n",
    "        blue_quat = np.array(self.euler_to_quat(\n",
    "            yaw=blue_orientation[0],\n",
    "            pitch=blue_orientation[1],\n",
    "            roll=blue_orientation[2],\n",
    "            degrees=False))\n",
    "        \n",
    "        # get agent's control state\n",
    "        if info is not None:\n",
    "            blue_ctrls = info['action_state']\n",
    "        else:\n",
    "            raise (ValueError, \"info parameter is required for this processor!\")\n",
    "        # -- old control stuff below --\n",
    "        #raw_ctrls = blue.controllers[0].get_applied_control()\n",
    "        # TODO: This should be handled by the action processor\n",
    "        #blue_ctrls = [raw_ctrls[0],\n",
    "        #              raw_ctrls[1],\n",
    "        #              raw_ctrls[2],\n",
    "        #              raw_ctrls[3] - 1]\n",
    "        \n",
    "        # get red's state information\n",
    "        red = platforms['red0']\n",
    "        \n",
    "        # TODO: modify this to be randomly set at each reset\n",
    "        # get goal location in UTM + altitude coordinates\n",
    "        goal_e, goal_n, goal_alt = get_goal_position(\n",
    "            platforms,\n",
    "            rel_bearing=0.0,\n",
    "            rel_range=6000.0/3.28084,  # 6 kft\n",
    "            rel_alt=300.0/3.28084)  # 300 ft\n",
    "        \n",
    "        # get agent's position in UTM coordinates\n",
    "        blue_ll = (blue.position[0], blue.position[1])\n",
    "        blue_e, blue_n, z, l = utm.from_latlon(blue_ll[0], blue_ll[1])\n",
    "        \n",
    "        # find utm coordinate of this distance\n",
    "        red_ll = (red.position[0], red.position[1])\n",
    "        # convert to utm\n",
    "        red_e, red_n, _, _ = utm.from_latlon(red_ll[0], red_ll[1], z, l) \n",
    "        \n",
    "        # get distance from blue to red\n",
    "        dist_r2b = np.sqrt((red_e - blue_e)**2 +\n",
    "                           (red_n - blue_n)**2 +\n",
    "                           (red.position[2] - blue.position[2])**2)\n",
    "        \n",
    "        # get distances from goal position\n",
    "        e_diff = blue_e - goal_e\n",
    "        n_diff = blue_n - goal_n\n",
    "        a_diff = blue.position[2] - goal_alt\n",
    "       \n",
    "        # get difference of distances from last time step\n",
    "        current_e_diff = abs(e_diff) - self._prev_e_diff\n",
    "        current_n_diff = abs(n_diff) - self._prev_n_diff\n",
    "        current_a_diff = abs(a_diff) - self._prev_a_diff\n",
    "       \n",
    "        # cache current difference for next frame\n",
    "        self._prev_e_diff = abs(e_diff)\n",
    "        self._prev_n_diff = abs(n_diff)\n",
    "        self._prev_a_diff = abs(a_diff)\n",
    "        \n",
    "        # get range from goal position\n",
    "        range_to_goal = np.sqrt((e_diff)**2 + (n_diff)**2 + (a_diff)**2)\n",
    "        \n",
    "        # get difference of range from last time step\n",
    "        current_r2g = range_to_goal - self._prev_r2g\n",
    "       \n",
    "        # cache range for next frame\n",
    "        self._prev_r2g = range_to_goal\n",
    "       \n",
    "        # get change rates of all distances\n",
    "        if current_time_diff == 0:\n",
    "            e_diff_rate = 0\n",
    "            n_diff_rate = 0\n",
    "            a_diff_rate = 0\n",
    "            r2g_rate = 0\n",
    "        else:\n",
    "            e_diff_rate = current_e_diff / current_time_diff\n",
    "            n_diff_rate = current_n_diff / current_time_diff\n",
    "            a_diff_rate = current_a_diff / current_time_diff\n",
    "            r2g_rate = current_r2g / current_time_diff\n",
    "        \n",
    "        \n",
    "        # Get angular rates of agent\n",
    "        blue_ang_vel = get_afsim_angular_rates(blue)\n",
    "        \n",
    "        # TODO: Get angular acceleration using finite difference (as function)\n",
    "        # something like\n",
    "        blue_ang_accel = get_afsim_angular_accel(blue_ang_vel,\n",
    "                                                 self._prev_ang_vel,\n",
    "                                                 current_time_diff)\n",
    "        \n",
    "        self._prev_ang_vel = blue_ang_vel\n",
    "        \n",
    "        self._prev_time = sim_time\n",
    "        \n",
    "        # get raw observations\n",
    "        raw_obs = {}\n",
    "        raw_obs['pose'] = blue_quat\n",
    "        raw_obs['altitude'] = blue_alt\n",
    "        raw_obs['controls'] = blue_ctrls\n",
    "        raw_obs['velocity'] = blue.velocity_ned\n",
    "        raw_obs['acceleration'] = blue.acceleration_ned\n",
    "        raw_obs['angular_velocity'] = blue_ang_vel\n",
    "        raw_obs['dist_from_goal'] = [e_diff, n_diff, a_diff]\n",
    "        raw_obs['dist_from_goal_rates'] = [e_diff_rate, n_diff_rate, a_diff_rate]\n",
    "        raw_obs['range_to_goal'] = range_to_goal\n",
    "        raw_obs['range_to_goal_rate'] = r2g_rate\n",
    "        raw_obs['lead_rel_velocity'] = red.velocity_ned - blue.velocity_ned\n",
    "        raw_obs['range_to_lead'] = dist_r2b\n",
    "        \n",
    "        # add raw observations to info\n",
    "        if info is not None:\n",
    "            info['raw_observation'] = raw_obs\n",
    "        else:\n",
    "            warnings.warn(\"Info not provided, raw observations will not be stored\",\n",
    "                          RuntimeWarning)\n",
    "        \n",
    "        # scale observations and return\n",
    "        obs = OrderedDict()\n",
    "        for k, v in raw_obs.items():\n",
    "            if isinstance(v, (list, tuple, np.ndarray)):\n",
    "                new_obs = []\n",
    "                for i, o in enumerate(v):\n",
    "                    if k in self._scale_factors.keys():\n",
    "                        new_obs.append(self.scale_obs(o, self._scale_factors[k][i]))\n",
    "                    else:\n",
    "                        new_obs.append(o)\n",
    "                obs[k] = np.array(new_obs)\n",
    "            else:\n",
    "                obs[k] = np.array([self.scale_obs(v, self._scale_factors[k])])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def scale_obs(self, obs, scale_factor=[0, 1]):\n",
    "        \"\"\"Scale observation by subtracting mean and dividing by variance.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : float\n",
    "            Observation to scale\n",
    "        \n",
    "        scale_factor : [float, float]\n",
    "            Tuple containing mean in [0] and variance in [1]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Scaled observation\n",
    "        \"\"\"\n",
    "        return (obs - scale_factor[0]) / scale_factor[1]\n",
    "    \n",
    "    # euler to quaternion\n",
    "    def euler_to_quat(self, yaw, pitch, roll, degrees=True):\n",
    "        \"\"\"Convert yaw, pitch and roll rotation to quaternion.\n",
    "\n",
    "        This function assumes that the angles are applied in\n",
    "        yaw, pitch, roll order.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        yaw : float\n",
    "            Rotation about the Y axis in degrees/radians\n",
    "        pitch : float\n",
    "            Rotation about the X axis in degrees/radians\n",
    "        roll : float\n",
    "            Rotation about the Z axis in degrees/radians\n",
    "        degrees : bool\n",
    "            If True, angles are assumed to be in degrees,\n",
    "            if false angles are assumed to be in radians\n",
    "        \"\"\"\n",
    "        seq = 'yxz'\n",
    "        angles = [yaw, pitch, roll]\n",
    "\n",
    "        rot = R.from_euler(seq=seq, angles=angles, degrees=degrees)\n",
    "\n",
    "        return rot.as_quat()\n",
    "    \n",
    "    # TODO: Add new observations (goal diff e, n, alt, vel/acc and angular vel/acc)\n",
    "    # and remove unused ones (red_rel_pose, etc.)\n",
    "    def get_observation_space(self, action_obs_space):\n",
    "        \"\"\"Get rllib observation space for the output of this processor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action_obs_space : spaces\n",
    "            Space of the observation component of the controls\n",
    "            used in the simulator. This isn't necessarily the\n",
    "            same as the action itself (e.g. for delta actions,\n",
    "            the action space is a delta, but we'd want to use\n",
    "            the actual control state of the aircraft in the\n",
    "            simulator).\n",
    "        \"\"\"\n",
    "        \n",
    "        obs_space = OrderedDict({\n",
    "           'pose': spaces.Box(-1, 1, shape=(4,)),\n",
    "           'altitude': spaces.Box(-5, 5, shape=(1,)),\n",
    "           # TODO: Probably need to pass in the action processor so it can\n",
    "           # this can be pulled in directly\n",
    "           #'controls': spaces.Box(low=np.array([-1, -1, -1, -1]),\n",
    "           #                       high=np.array([1, 1, 1, 1]),\n",
    "           #                       dtype=np.float32),\n",
    "           'controls': action_obs_space,\n",
    "           'velocity': spaces.Box(-10, 10, shape=(3,)),\n",
    "           'acceleration': spaces.Box(-10, 10, shape=(3,)),\n",
    "           'angular_velocity': spaces.Box(-10, 10, shape=(3,)),\n",
    "           'dist_from_goal': spaces.Box(-10, 10, shape=(3,)),\n",
    "           'dist_from_goal_rates': spaces.Box(-10, 10, shape=(3,)),\n",
    "           'range_to_goal': spaces.Box(-10, 10, shape=(1,)),\n",
    "           'range_to_goal_rate': spaces.Box(-10, 10, shape=(1,)),\n",
    "           'lead_rel_velocity': spaces.Box(-10, 10, shape=(3,)),\n",
    "           'range_to_lead': spaces.Box(-10, 10, shape=(1,)),\n",
    "        })\n",
    "        \n",
    "        return spaces.Dict(obs_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal Position Helper Function\n",
    "\n",
    "This helper function computes the rejoin goal location in UTM and Altitude coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Current rejoin success parameters:\n",
    "\n",
    "CheckRejoinDone:\n",
    "    # The radius in feet that the targe join need to occur, by default 100\n",
    "    radius : 750\n",
    "    # The time rate in ft/s for range change, by default 10\n",
    "    capture_rate : 32\n",
    "    # the target az from the lead, by default 0\n",
    "    target_az : 0\n",
    "    # the target slant range from the lead, by default 6000\n",
    "    target_slant_range : 6000\n",
    "    # The target elevation from the lead, by default 300\n",
    "    target_elevation : 300\n",
    "    # Time to check that rejoin has happend in seconds, by default 2\n",
    "    time_in_bubble : 2\n",
    "\"\"\"\n",
    "# TODO: This is pretty inefficient and could be refactored into a class at some point\n",
    "def get_goal_position(sim_state,\n",
    "                      rel_bearing=0.0,\n",
    "                      rel_range=6000,\n",
    "                      rel_alt=300):\n",
    "    \"\"\"Calculate goal location from state of lead aircraft in ENU.\n",
    "    \n",
    "    This assumes East is the z axis of the aircraft (e.g forward) and\n",
    "    North is the x axis of the aircraft (e.g. right). Heading is assumed\n",
    "    to be 0 when pointing directly East.\n",
    "    TODO: This convention might need to be flipped if the heading\n",
    "    is different - need to confirm ENU coordinate frame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sim_state : StateDict\n",
    "        StateDict object containing entries for each entity in\n",
    "        the simulation. For this environment, it is assumed to\n",
    "        contain a blue0 (our agent) and red0 (the lead aircraft).\n",
    "        \n",
    "    rel_bearing : float\n",
    "        Bearing from the goal position to the lead aircraft in\n",
    "        degrees\n",
    "    \n",
    "    rel_range : float\n",
    "        Range in East, North plane from goal position to the\n",
    "        lead aircraft in meters\n",
    "        \n",
    "    rel_alt : float\n",
    "        Altitude difference between goal position and the\n",
    "        lead aircraft\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float, float]\n",
    "        A tuple containing the goal UTM easting location, the goal\n",
    "        UTM northing location and the goal altitude\n",
    "    \"\"\"\n",
    "    # get lead aircraft's heading\n",
    "    heading = sim_state['red0'].orientation[0]\n",
    "    # convert to degrees\n",
    "    heading *= 180 / math.pi\n",
    "    \n",
    "    # get angle from rejoin point to lead\n",
    "    angle = ((rel_bearing + heading) % 360)\n",
    "    \n",
    "    # find e, n location in meters\n",
    "    e = rel_range * math.sin(math.radians(angle))  # north is heading = 0.0\n",
    "    n = rel_range * math.cos(math.radians(angle))  # north is heading = 0.0\n",
    "    #e = rel_range * math.cos(math.radians(angle))  # east is heading = 0.0\n",
    "    #n = rel_range * math.sin(math.radians(angle))  # east is heading = 0.0\n",
    "        \n",
    "    # get agent's utm zone\n",
    "    blue_ll = (sim_state['blue0'].position[0],\n",
    "               sim_state['blue0'].position[1])\n",
    "    blue_e, blue_n, z, l = utm.from_latlon(blue_ll[0], blue_ll[1])\n",
    "    # find utm coordinate of this distance\n",
    "    red_ll = (sim_state['red0'].position[0], sim_state['red0'].position[1])\n",
    "    # convert to utm\n",
    "    red_e, red_n, _, _ = utm.from_latlon(red_ll[0], red_ll[1], z, l) \n",
    "    \n",
    "    goal_e = red_e - e\n",
    "    goal_n = red_n - n\n",
    "    \n",
    "    # get altitude of goal position\n",
    "    goal_alt = sim_state['red0'].position[2] + rel_alt\n",
    "    \n",
    "    return goal_e, goal_n, goal_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Angular Rates from AfsimPlatform Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_afsim_angular_rates(platform):\n",
    "    \"\"\"Get angular rates from afsim platform.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    platform : AfsimPlatform\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float, float]\n",
    "        Yaw, pitch and roll rates\n",
    "    \"\"\"\n",
    "    mover, _, _, _ = platform._platform.get_components()\n",
    "    \n",
    "    return mover.get_yaw_rate(), mover.get_pitch_rate(), mover.get_roll_rate()\n",
    "\n",
    "# TODO: Implement angular accelerations function\n",
    "def get_afsim_angular_accel(ang_vel,\n",
    "                            prev_ang_vel,\n",
    "                            time_diff):\n",
    "    \"\"\"Estimate angular acceleration through finite difference method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ang_vel : [float, float, float]\n",
    "        Angular velocity of current frame\n",
    "    prev_ang_vel : [float, float, float]\n",
    "        Angular velocity at time - time_diff\n",
    "    time_diff : float\n",
    "        Difference in seconds between measurements\n",
    "    \"\"\"\n",
    "    if time_diff > 0:\n",
    "        return [(t - p) / time_diff for t, p in zip(ang_vel, prev_ang_vel)]\n",
    "    else:\n",
    "        return [0.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Support Classes\n",
    "\n",
    "Here we create helper classes that will calculate the reward for each step, determine if the agent is done and an action processor that will take the action output by the rllib policy network and apply it to the AFSIM simulator.\n",
    "\n",
    "Current there is a reward calculator class, a done checking class and an action processor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RejoinRewardCalculator:\n",
    "    \"\"\"Class to compute rewards for agent performing rejoin task.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize the calculator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        config : dict\n",
    "            Dictionary containing parameters for the \n",
    "            various reward calculations performed by\n",
    "            this object\n",
    "        \"\"\"\n",
    "        # TODO: Grab all this from config\n",
    "        self._step_reward = 0.0005\n",
    "       \n",
    "        # acceptable distance from goal to be in success 'bubble'\n",
    "        self._bubble = [100, 100, 100]\n",
    "        \n",
    "        # warning zones\n",
    "        self._min_alt = 500\n",
    "        self._max_alt = 45000\n",
    "        self._min_range = 300\n",
    "        self._max_range = 18000\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the state of this object.\"\"\"\n",
    "        # TODO: Return all parameters to their initial values,\n",
    "        # this probably won't do anything\n",
    "        \n",
    "    def __call__(self, info):\n",
    "        \"\"\"Compute the reward given the environment information.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        info : dict\n",
    "            Dictionary containing information on the state of\n",
    "            the environment, scenario, agent, etc. \n",
    "            TODO: What should be in here for this specific\n",
    "            object?\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "       \n",
    "        # TODO: scale lateral and altitude changes seperately\n",
    "        \n",
    "        # if moving toward goal, award self._step_reward\n",
    "        #if info['raw_observation']['range_to_goal_rate'] <= 0:\n",
    "            #reward = self._step_reward\n",
    "        reward = -0.00001 * info['raw_observation']['range_to_goal_rate']\n",
    "                \n",
    "        # if inside success bubble, award 2 * self._step_reward\n",
    "        # NOTE: bubble is pretty small right now\n",
    "        # TODO: Make sure this is much larger than the distance toward\n",
    "        # goal reward, basically should be 2x the largest distance toward\n",
    "        # reward\n",
    "        dists = info['raw_observation']['dist_from_goal']\n",
    "        if (abs(dists[0]) <= self._bubble[0] and\n",
    "                abs(dists[1]) <= self._bubble[1] and\n",
    "                abs(dists[2]) <= self._bubble[2]):\n",
    "            #reward = 2 * self._step_reward\n",
    "            reward = 10 * self._step_reward\n",
    "            info['num_steps_inside_bubble'] += 1\n",
    "        \n",
    "        # if nearing mission altitude bounds, penalize\n",
    "        alt = info['raw_observation']['altitude']\n",
    "        if (alt < self._min_alt) or (alt > self._max_alt):\n",
    "            reward = -3 * self._step_reward\n",
    "       \n",
    "        # if nearing mission range bounds, penalize\n",
    "        dist = info['raw_observation']['range_to_lead']\n",
    "        if (dist < self._min_range) or (dist > self._max_range):\n",
    "            reward = -3 * self._step_reward\n",
    "            \n",
    "        return reward\n",
    "        \n",
    "class RejoinDoneChecker:\n",
    "    \"\"\"\n",
    "    Class to check of the agent has met done conditions\n",
    "    for the Rejoin task.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize the calculator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        config : dict\n",
    "            Dictionary containing parameters for the \n",
    "            various reward calculations performed by\n",
    "            this object\n",
    "        \"\"\"\n",
    "        # TODO: actually use a configuration\n",
    "        self._min_alt = 100\n",
    "        self._max_alt = 50000\n",
    "        \n",
    "        self._min_range = 100\n",
    "        self._max_range = 20000\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the state of this object.\"\"\"\n",
    "        # TODO: Return all parameters to their initial values,\n",
    "        # might not need anything\n",
    "        \n",
    "    def __call__(self, info):\n",
    "        \"\"\"\n",
    "        Determine if the agent is done based on the information\n",
    "        provided on the state of the evironment.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        info : dict\n",
    "            Dictionary containing information on the state of\n",
    "            the environment, scenario, agent, etc. \n",
    "            TODO: What should be in here for this specific\n",
    "            object?\n",
    "        \"\"\"\n",
    "        # Notes: right now this can just check to see if the aircraft\n",
    "        # is outside some mission bounds as defined in the config and\n",
    "        # if the number of steps is greater than horizon. Could return\n",
    "        # the done state in the info if desired.\n",
    "        step_done = info['current_step'] > info['horizon']\n",
    "        \n",
    "        # check if agent is outside altitude bounds\n",
    "        alt = info['raw_observation']['altitude']\n",
    "        alt_done = alt < self._min_alt or alt > self._max_alt\n",
    "       \n",
    "        # check if agent is outside of range bounds\n",
    "        # TODO: Get range to lead aircraft for min range check\n",
    "        dist = info['raw_observation']['range_to_lead']\n",
    "        dist_done = dist < self._min_range or dist > self._max_range\n",
    "        \n",
    "        done_states = {'step_done': step_done,\n",
    "                       'alt_done': alt_done,\n",
    "                       'dist_done': dist_done}\n",
    "        \n",
    "        info['done_states'] = done_states\n",
    "        \n",
    "        if (step_done or alt_done or dist_done):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        return done\n",
    "    \n",
    "class AfsimHOTASController:\n",
    "    \"\"\"Class to apply HOTAS commands to AFSIM.\n",
    "    \n",
    "    This class handles all aspects of the action. It applies the\n",
    "    action supplied by the policy to the simulation, provides the\n",
    "    action space to the policy and provides the observation and\n",
    "    observation space to the observation processor if state of the\n",
    "    control is desired to be a part of the observation.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Setup the control processor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : dict\n",
    "            Dictionary containing the names of the agents\n",
    "            that this controller will send commands for\n",
    "            TODO: should this be env_config???\n",
    "        \"\"\"\n",
    "        # TODO: actually use a configuration\n",
    "        self._agent_name = 'blue0'\n",
    "\n",
    "    def get_action_space(self):\n",
    "        \"\"\"Get the action space of this object.\n",
    "\n",
    "        Returns:\n",
    "            spaces.Box\n",
    "            The action space that this object uses\n",
    "        \"\"\"\n",
    "        return spaces.Box(low=np.array([-1, -1, -1, -1]),\n",
    "                          high=np.array([1, 1, 1, 1]),\n",
    "                          dtype=np.float32)\n",
    "    \n",
    "    def get_observation_space(self):\n",
    "        \"\"\"Get the observation space of the controller.\n",
    "\n",
    "        Returns:\n",
    "            spaces.Box\n",
    "            The observation space that this object uses\n",
    "        \"\"\"\n",
    "        return spaces.Box(low=np.array([-1, -1, -1, -1]),\n",
    "                          high=np.array([1, 1, 1, 1]),\n",
    "                          dtype=np.float32)\n",
    "    \n",
    "    def get_observation(self, platforms):\n",
    "        \"\"\"Get the observation of the controller.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        platforms : Tuple[AfsimPlatform, ...]\n",
    "            Tuple of AfsimPlatform objects, with an\n",
    "            entry for each platform in the scene\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[float, float, float, float]\n",
    "            The state of the control of the platform.\n",
    "        \"\"\"\n",
    "        ctrl = None\n",
    "        for p in platforms:\n",
    "            if p.name == self._agent_name:\n",
    "                ctrl = p.controllers[0].get_applied_control()\n",
    "       \n",
    "        if ctrl is None:\n",
    "            raise (ValueError, f\"Control not available for {self._agent_name}!\")\n",
    "            \n",
    "        agent_ctrls = [ctrl[0],\n",
    "                       ctrl[1],\n",
    "                       ctrl[2],\n",
    "                       ctrl[3] - 1]\n",
    "        \n",
    "        return agent_ctrls\n",
    "\n",
    "    def __call__(self, platforms, action):\n",
    "        \"\"\"Process the rllib action and apply in AFSIM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        platforms : Tuple[AfsimPlatform, ...]\n",
    "            Tuple of AfsimPlatform objects, with an\n",
    "            entry for each platform in the scene \n",
    "            \n",
    "        action : np.array\n",
    "            Numpy array of action calculation from rllib's\n",
    "            agent.compute_action method\n",
    "        \"\"\"\n",
    "        action = flatten_to_single_ndarray(action)\n",
    "        # get action for afsim\n",
    "        aileron = action[0]\n",
    "        elevator = action[1]\n",
    "        rudder = action[2]\n",
    "        throttle = (action[3] + 1) / 2  # no afterburner\n",
    "        \n",
    "        sim_action = {self._agent_name: [aileron, elevator, rudder, throttle]}\n",
    "       \n",
    "        # Note: this is pretty simple because we only have one controller,\n",
    "        # if there were multiple, we would need a second for loop to loop\n",
    "        # over the controllers and get the name of the one we care about\n",
    "        for p in platforms:\n",
    "            if p.name == self._agent_name:\n",
    "                ctrl = p.controllers[0]\n",
    "                ctrl.apply_control(np.array(sim_action[self._agent_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializer\n",
    "\n",
    "Here we define a helper class that will generate random starting states for the agent and lead platforms in the environment. We can use this at each reset to randomly initialize each aircraft according to rules defined in the env_config. This is a standalone version of what is embedded into the RejoinChallenge and base AACO Environment classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import afsim\n",
    "\n",
    "from act3.environments.util.env_common import EnvCommonValues\n",
    "from act3.environments.util.platform_centric_utils import PlatformCentricUtils\n",
    "\n",
    "\n",
    "class RejoinInitialConditions:\n",
    "    \"\"\"Class to set starting state of AFSIM platforms.\"\"\"\n",
    "    \n",
    "    _BLUE0 = \"blue0\"\n",
    "    _RED0 = \"red0\"\n",
    "    _LEAD_LAT = \"lead_lat\"\n",
    "    _LEAD_LON = \"lead_lon\"\n",
    "    _LEAD_ALT = \"lead_alt\"\n",
    "    _LEAD_HEADING = \"lead_heading\"\n",
    "    _LEAD_HEADING_DEF = 180.0\n",
    "    _LEAD_MAN_ROLL = \"lead_manuever_roll\"\n",
    "    _LEAD_ROLL_MAN_DEF = 0\n",
    "    _LEAD_KCAS_MAN = \"lead_manuever_kcas\"\n",
    "    _LEAD_KCAS_MAN_DEF = 350\n",
    "    _FOLLOW_HEADING = \"follow_heading\"\n",
    "    _SET_ROLL_ANGLE_CMD = \"SetRollAngle\"\n",
    "    _SET_ALTITUDE_CMD = \"SetAltitude\"\n",
    "    _LEAD_ALT_MAN = \"lead_manuever_altitude\"\n",
    "    _SET_KCAS_CMD = \"SetKCAS\"\n",
    "    _DEFAULT_VELOCITY_NED = 180.0  # m/s : 350 KCAS\n",
    "    _TRAIL_ALT_DIFF = \"trail_alt_diff\"\n",
    "    _TRAIL_ALT_DIFF_DEF = 0\n",
    "    _START_RANGE = \"start_range\"\n",
    "    _START_RANGE_DEF = 11482\n",
    "    _TRAIL_AZIMUTH = \"trail_azimuth\"\n",
    "    _TRAIL_AZIMUTH_DEF = 0\n",
    "    \n",
    "    def __init__(self, env_config):\n",
    "        \"\"\"Initialize udr module\"\"\"\n",
    "        self._env_config = env_config\n",
    "        self.dr_module = DomainRandomization(env_config)\n",
    "        self.phase = env_config['domain_randomization_config']['randomizing_phase']\n",
    "        \n",
    "    def set_phase(self, phase):\n",
    "        \"\"\"Set the phase of the domain randomization module.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        phase : int\n",
    "            Phase of the domain randomization module to use.\n",
    "        \"\"\"\n",
    "        env_config = self._env_config\n",
    "        env_config['domain_randomization_config']['randomizing_phase'] = phase\n",
    "        self.phase = phase\n",
    "        self.dr_module = DomainRandomization(env_config)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        \"\"\"Set the initial state of the aircraft.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : Tuple[AfsimPlatform, ...]\n",
    "            Tuple of AfsimPlatform objects, with an\n",
    "            entry for each platform in the scene\n",
    "        \"\"\"\n",
    "        params = self.dr_module.sample()\n",
    "        lead_lat = params[self._LEAD_LAT]\n",
    "        lead_lon = params[self._LEAD_LON]\n",
    "        lead_alt_f = params[self._LEAD_ALT]\n",
    "        lead_alt_m = lead_alt_f * EnvCommonValues.FEET_TO_METERS\n",
    "        red_heading_deg = params.get(self._LEAD_HEADING, self._LEAD_HEADING_DEF)\n",
    "        red_heading_rad = afsim.Util.degrees_to_radians(red_heading_deg)\n",
    "        lead_roll_angle_deg = params.get(self._LEAD_MAN_ROLL, self._LEAD_ROLL_MAN_DEF)\n",
    "        lead_manuever_kcas = params.get(self._LEAD_KCAS_MAN, self._LEAD_KCAS_MAN_DEF)\n",
    "        # currently not in env_config.yml phase 1-8, will always use lead_alt_f\n",
    "        lead_manuever_alt = params.get(self._LEAD_ALT_MAN, lead_alt_f)\n",
    "        b0_alt_diff = self.get_b0_alt_diff(params)\n",
    "        b0_slant_range = self.get_b0_slant_range(params)\n",
    "        b0_azimuth = self.get_b0_azimuth(params, red_heading_deg)\n",
    "\n",
    "        blue_lat, blue_lon, _ = PlatformCentricUtils.aer2geodetic(\n",
    "            az=b0_azimuth,\n",
    "            el=0,\n",
    "            srange=b0_slant_range,\n",
    "            lat0=lead_lat,\n",
    "            lon0=lead_lon,\n",
    "            h0=lead_alt_m,\n",
    "        )\n",
    "\n",
    "        # blue0 defaults to pointing at red0, but may have it's own\n",
    "        b0_heading_deg = params.get(self._FOLLOW_HEADING, (b0_azimuth - 180) % 360)\n",
    "        b0_heading_rad = afsim.Util.degrees_to_radians(b0_heading_deg)\n",
    "        for platform in state:\n",
    "            p = platform._platform  # pylint: disable=W0212\n",
    "            if platform.name == self._RED0:\n",
    "                p.set_location_lla(lead_lat, lead_lon, lead_alt_m)\n",
    "                vel_n, vel_e, vel_d = self.calculate_init_velocity_component(\n",
    "                    red_heading_rad, self._DEFAULT_VELOCITY_NED\n",
    "                )\n",
    "                p.set_orientation_ned(red_heading_rad, 0, 0)\n",
    "                p.set_velocity_ned(np.array([vel_n, vel_e, vel_d]))\n",
    "                afsim.util.Util.update_execute_function(\n",
    "                    p, self._SET_ALTITUDE_CMD, lead_manuever_alt\n",
    "                )\n",
    "                afsim.util.Util.update_execute_function(\n",
    "                    p, self._SET_ROLL_ANGLE_CMD, lead_roll_angle_deg\n",
    "                )\n",
    "                afsim.util.Util.update_execute_function(\n",
    "                    p, self._SET_KCAS_CMD, lead_manuever_kcas\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                p.set_location_lla(\n",
    "                    blue_lat, blue_lon, lead_alt_m + b0_alt_diff\n",
    "                )\n",
    "                p.set_orientation_ned(\n",
    "                    b0_heading_rad, 0, 0\n",
    "                )  # TODO: get actual heading for blue\n",
    "                vel_n, vel_e, vel_d = self.calculate_init_velocity_component(\n",
    "                    b0_heading_rad, self._DEFAULT_VELOCITY_NED\n",
    "                )\n",
    "                p.set_velocity_ned(np.array([vel_n, vel_e, vel_d]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_init_velocity_component(heading: float, speed: float):\n",
    "        \"\"\"\n",
    "        calculate_init_velocity_component takes in a heading a speed and then\n",
    "        calculates the NED velocity components from a speed value\n",
    "\n",
    "        Arguments:\n",
    "            heading: float\n",
    "                the heading direction for this aircraft in radians\n",
    "            speed: float\n",
    "                the speed for this aircraft\n",
    "        Returns\n",
    "        -------\n",
    "        vel_n: float\n",
    "            The north component in the velocity vector\n",
    "        vel_e: float\n",
    "            The east component of the velocity vector\n",
    "        vel_d: float\n",
    "            The down component of the velocity vector (for now always returns 0)\n",
    "        \"\"\"\n",
    "        # THESE ARE REVERSED ON PURPOSE, vel_e is vel_n and viceversa because\n",
    "        # of afsim orientation, N,E,S,W=0,90,180,270 so return opposite values\n",
    "        vel_e = np.sin(heading) * speed\n",
    "        vel_n = np.cos(heading) * speed\n",
    "        return vel_n, vel_e, 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_b0_alt_diff(params: dict) -> float:\n",
    "        \"\"\"\n",
    "        get_b0_alt_diff gets the altitude difference between the red0 and blue0\n",
    "        agent\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : typing.Dict\n",
    "            DR module env parameters,\n",
    "            may contain \"trail_alt_diff\"\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        b0_alt_diff: float\n",
    "            The Altitude difference between the red0 agent and the blue0 agent\n",
    "        \"\"\"\n",
    "        b0_alt_diff = params.get(\n",
    "            RejoinInitialConditions._TRAIL_ALT_DIFF, RejoinInitialConditions._TRAIL_ALT_DIFF_DEF\n",
    "        )\n",
    "        return b0_alt_diff * EnvCommonValues.FEET_TO_METERS\n",
    "\n",
    "    @staticmethod\n",
    "    def get_b0_slant_range(params: dict) -> float:\n",
    "        \"\"\"\n",
    "        get_b0_slant_range gets the slant range between the red0 and blue0\n",
    "        agent\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : typing.Dict\n",
    "            DR module env parameters,\n",
    "            may contain \"start_range\"\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        b0_slant_range: float\n",
    "            The slant range difference between the red0 agent and the blue0 agent\n",
    "        \"\"\"\n",
    "        b0_slant_range = params.get(\n",
    "            RejoinInitialConditions._START_RANGE, RejoinInitialConditions._START_RANGE_DEF\n",
    "        )\n",
    "        return b0_slant_range * EnvCommonValues.FEET_TO_METERS\n",
    "\n",
    "    @staticmethod\n",
    "    def get_b0_azimuth(params: dict, red_heading_deg: float) -> float:\n",
    "        \"\"\"\n",
    "        get_b0_azimuth gets the azimuth between the red0 and blue0\n",
    "        agent\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : typing.Dict\n",
    "            DR module env parameters,\n",
    "            may contain \"trail_azimuth\"\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        b0_azimuth: float\n",
    "            The azimuth difference between the red0 agent and the blue0 agent\n",
    "        \"\"\"\n",
    "        b0_aspect_angle = params.get(\n",
    "            RejoinInitialConditions._TRAIL_AZIMUTH, RejoinInitialConditions._TRAIL_AZIMUTH_DEF\n",
    "        )\n",
    "        azimuth = ((red_heading_deg - 180) % 360 + (b0_aspect_angle * -1)) % 360\n",
    "        # pymap3d seems to take the abs of the azimuth\n",
    "        # so need to get proper angle\n",
    "        b0_azimuth = (360 + azimuth) % 360\n",
    "        return b0_azimuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Policy \n",
    "\n",
    "Here we create the policy. First, we load in the AACO environment configurations yaml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation Configuration\n",
    "# sim_config_yml = ('/proj/aaco/team/ben/projects/act3-rllib-agents/'\n",
    "#                   'config/tasks/rejoin_UDR/sim_config.yml')\n",
    "sim_config_yml = ('/proj/aaco/team/ben/projects/act3-rllib-agents/config/tasks/rejoin/sim_config.yml')\n",
    "env_config_yml = ('/proj/aaco/team/ben/projects/act3-rllib-agents/config/tasks/rejoin/env_config.yml')\n",
    "\n",
    "\n",
    "with open(sim_config_yml) as f:\n",
    "    sim_config = yaml.safe_load(f)\n",
    "\n",
    "from act3.agents.utilities.yaml_loader import Loader, construct_include\n",
    "\n",
    "yaml.add_constructor(\"!include\", construct_include, Loader)\n",
    "\n",
    "with open(env_config_yml) as f:\n",
    "    tmp_env_config = yaml.load(f, Loader)\n",
    "    \n",
    "#config = {'output_path': '~/ray_results'}\n",
    "env_config = tmp_env_config['environment']\n",
    "#env_config['sim_config'] = sim_config['scen']\n",
    "env_config['sim_config'] = sim_config\n",
    "env_config['sim_config']['scen']['frame_rate'] = 10  # set the frame rate of the simulation\n",
    "# All output of training will be placed into the specified directory\n",
    "# TODO: This used to place all of the checkpoints into this directory as well, but \n",
    "# that seems to have changed. Need to figure out how to fix that.\n",
    "env_config['sim_config']['scen']['output_path'] = 'output_simple_rejoin_082620_128x128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config['simulator'] = 'AfsimIntegration'  # this might not be needed any longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Configuration\n",
    "# set max number of steps for an episode\n",
    "env_config['horizon'] = 1000\n",
    "# set the step reward magnitude for the environment\n",
    "#env_config['step_reward'] = 1.0 / float(env_config['horizon'])\n",
    "env_config['step_reward'] = 0.4 / float(env_config['horizon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "# policy training parameters for PPO\n",
    "trainer_config = DEFAULT_CONFIG.copy()\n",
    "trainer_config['num_workers'] = 12\n",
    "trainer_config['train_batch_size'] = 12000\n",
    "trainer_config['gamma'] = 0.995\n",
    "trainer_config['lambda'] = 0.9  # from googlebrain paper\n",
    "trainer_config['rollout_fragment_length'] = 1000\n",
    "trainer_config['batch_mode'] = 'complete_episodes'\n",
    "trainer_config['sgd_minibatch_size'] = 4000 \n",
    "trainer_config['clip_param'] = 0.3\n",
    "trainer_config['vf_clip_param'] = 10.0\n",
    "trainer_config['model']['vf_share_layers'] = False  # from googlebrain paper\n",
    "# trainer_config['num_sgd_iter'] = 10\n",
    "trainer_config['num_sgd_iter'] = 30\n",
    "trainer_config['lr'] = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this network is pretty compact, might need to be beefed up\n",
    "# trainer_config['model']['fcnet_hiddens'] = [32, 32]  # from 8-26-20 (didn't seem to work well after 2 hours training)\n",
    "trainer_config['model']['fcnet_hiddens'] = [128, 128]  # from 08-25-20 (output_simple_rejoin_test_v2)\n",
    "# trainer_config['model']['fcnet_hiddens'] = [256, 256, 256]  # from 8-25-20 (output_simple_rejoin_256x256x256) did not look good\n",
    "#trainer_config['model']['free_log_std'] = True\n",
    "#trainer_config['model']['use_lstm'] = True  # use with smaller model [64, 64] or [32, 32]\n",
    "#trainer_config['model']['max_seq_len'] = 50\n",
    "#trainer_config['model']['lstm_use_prev_action_reward'] = True\n",
    "# environment configuration\n",
    "trainer_config['env_config'] = env_config\n",
    "# probably don't need this stuff anymore\n",
    "trainer_config['env_config']['sim_config']['scen']['blue_team'][0]['route'] = [[39.8537, -84.0537], [39.7137, -84.0537]]\n",
    "trainer_config['env_config']['sim_config']['scen']['blue_team'][0]['position'] = [39.8537, -84.0537]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Policy With Backtracking and Reward Collapse Protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Output Path for Checkpoints\n",
    "\n",
    "# for local testing this will place checkpoints in the same parent folder as the aer files\n",
    "# this probably needs to be changed for docker container to something like the below\n",
    "#ray_results = '/home/aaco/ray_results/ACT3-RLLIB-AGENTS/'  # for docker container\n",
    "ray_results = './'  \n",
    "output_path = (ray_results + trainer_config['env_config']['sim_config']['scen']['output_path']\n",
    "               + '/checkpoints')\n",
    "\n",
    "# Check if path exists. If not create it\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create PPO Agent\n",
    "trainer = PPOTrainer(config=trainer_config, env=RejoinDREnv)\n",
    "\n",
    "# Restore old checkpoint if desired\n",
    "# trainer.restore(output_path + '/checkpoint_900/checkpoint-900')\n",
    "\n",
    "# Setup Training Parameters\n",
    "initial_lr = trainer_config['lr']\n",
    "initial_thresh = 1000\n",
    "drop_thresh = 100\n",
    "best_reward_mean = -10.0\n",
    "counter = 0\n",
    "thresh = 1000\n",
    "min_lr = 5e-10\n",
    "lr_decrement = 10\n",
    "max_iter = 2e7\n",
    "# max_iter = 5  # for testing\n",
    "prev_worse = False\n",
    "vcount = 0\n",
    "i = 0\n",
    "\n",
    "while trainer_config['lr'] >= min_lr and i < max_iter:\n",
    "    print(f'Training iteration {i}...')\n",
    "    result = trainer.train()\n",
    "    ep_mean = result['episode_reward_mean']\n",
    "    print(f'Episode reward mean is {ep_mean} for training iteration {i}...')\n",
    "    # checkpoint model if mean reward is best so far\n",
    "    if result['episode_reward_mean'] >= best_reward_mean:\n",
    "        checkpoint_path = trainer.save(output_path)\n",
    "        best_reward_checkpoint = checkpoint_path\n",
    "        best_reward_mean = result['episode_reward_mean']\n",
    "        print(f'Checkpoint {i} saved to {checkpoint_path}')\n",
    "        print(pretty_print(result))\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    # check to see if reward is falling\n",
    "    if ((ep_mean - best_reward_mean) / abs(best_reward_mean)) < -0.2:\n",
    "        print(f'Iteration {i} had a mean reward more than 20% worse '\n",
    "              f'then the current best reward!')\n",
    "        print(f'Mean reward was {ep_mean} and best is {best_reward_mean}...')\n",
    "        if prev_worse:\n",
    "            vcount += 1\n",
    "        prev_worse = True\n",
    "    else:\n",
    "        prev_worse = False\n",
    "        vcount = 0\n",
    "        \n",
    "    if trainer_config['lr'] == initial_lr:\n",
    "        fall_thresh = initial_thresh\n",
    "    else:\n",
    "        fall_thresh = thresh\n",
    "    # grace period for falling reward\n",
    "    if (counter > fall_thresh) or (vcount > drop_thresh):\n",
    "        print('Reducing learning rate!')\n",
    "        print(f'Steps without reward increase was {counter}'\n",
    "              f'and consecutive steps with reward drop was {vcount}')\n",
    "        print('Resuming from previous best checkpoint and '\n",
    "              'decrementing learning rate...')\n",
    "        trainer_config['lr'] /= lr_decrement\n",
    "        \n",
    "        if vcount > drop_thresh: \n",
    "            # only backtrack if reward collapse was detected\n",
    "            trainer = PPOTrainer(config=trainer_config, env=RejoinDREnv)\n",
    "            trainer.restore(best_reward_checkpoint)\n",
    "            vcount = 0\n",
    "        else:\n",
    "            # otherwise restore current state with lower learning rate\n",
    "            checkpoint_path = trainer.save(output_path)\n",
    "            trainer = PPOTrainer(config=trainer_config, env=RejoinDREnv)\n",
    "            trainer.restore(checkpoint_path)\n",
    "            vcount = 0\n",
    "            \n",
    "        counter = 0\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save environment configuration as pickle file\n",
    "This mimics what ray does for its checkpoints and enables the environment to be used by the\n",
    "AACO evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "params_file = (trainer.config['env_config']['sim_config']['scen']['output_path'] + '/checkpoints/params.pkl')\n",
    "\n",
    "trainer.config['env'] = 'RejoinDREnv-v0'\n",
    "\n",
    "pickle.dump(trainer.config, open(params_file, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Evaluation\n",
    "\n",
    "This might need to be updated. Haven't tried using it with the evaluator since June."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from act3.agents.evaluation import RejoinEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#checkpoint_path = '/home/aaco/ray_results/ACT3-RLLIB-AGENTS/PPO_RejoinChallenge_0_2020-07-15_03-05-397mn8482f'\n",
    "experiment_path = None\n",
    "params_file = os.path.join(experiment_path,'params.pkl') \n",
    "\n",
    "params = pickle.load( open( params_file, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register environment\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "register_env(\"RejoinEnv-v0\", lambda config: MatchAltEnv(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart ray\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False, temp_dir=temp_dir, webui_host='127.0.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RejoinEvaluator(experiment_path=experiment_path, ray_temp=temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.set_phase(phase=2)\n",
    "\n",
    "metrics = evaluator.evaluate(num_episodes=3, visualize_episodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics[0]['blue0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[2]['blue0']['num_steps']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aaco-rl] *",
   "language": "python",
   "name": "conda-env-aaco-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
